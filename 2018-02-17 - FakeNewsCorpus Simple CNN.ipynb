{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import multiprocessing\n",
    "\n",
    "import ujson\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524288000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(500 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'data/fake_news_corpus/'\n",
    "path_news_csv = path_data + 'news_cleaned_2018_02_13.csv'\n",
    "path_fasttext = path_data + 'news_cleaned_2018_02_13.fasttext.bin'\n",
    "path_news_preprocessed = path_data + 'news_cleaned_2018_02_13.preprocessed.jsonl'\n",
    "path_news_shuffled = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.jsonl'\n",
    "path_news_train = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.train.jsonl'\n",
    "path_news_test = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.test.jsonl'\n",
    "path_news_val = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.val.jsonl'\n",
    "\n",
    "path_news_embedded = path_data + 'news_cleaned_2018_02_13.embedded.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText.load_fasttext_format(path_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load news_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_news_chunk in pd.read_csv(path_news_csv, chunksize=1000):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the input data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the file is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm() as progress:\n",
    "    for df_news_chunk in pd.read_csv(path_news_csv, encoding='utf-8', engine='python', chunksize=1000):\n",
    "        progress.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_generator():\n",
    "    with tqdm() as progress:\n",
    "        for df_news_chunk in pd.read_csv(path_news_csv, encoding='utf-8', engine='python', chunksize=10 * 1000):\n",
    "            news_filter = df_news_chunk.type.isin(set(['fake', 'conspiracy', 'unreliable', 'reliable']))\n",
    "            df_news_chunk_filtered = df_news_chunk[news_filter]\n",
    "            for row in df_news_chunk_filtered.itertuples():\n",
    "                label = 1 if row.type == 'reliable' else 0\n",
    "\n",
    "                progress.update()\n",
    "                yield int(row.id), '%s %s' % (row.title, row.content), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show text length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for i, (_id, con, label) in enumerate(news_generator()):\n",
    "    if i > 10 * 1000:\n",
    "        break\n",
    "\n",
    "    lens.append(len(con))\n",
    "    progress.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lens), sum(lens) / len(lens), max(lens), min(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c5ba861c35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lens' is not defined"
     ]
    }
   ],
   "source": [
    "sns.distplot(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_string(news):\n",
    "    _id, con, label = news\n",
    "    return _id, preprocess_string(con), label\n",
    "\n",
    "def news_preprocessed_generator():\n",
    "    missing_words = {}\n",
    "    \n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count(), maxtasksperchild=1) as pool:\n",
    "        for _id, con, label in pool.imap(_preprocess_string, news_generator(), chunksize=1000):\n",
    "            yield _id, con, label, missing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing_words = {}\n",
    "with open(path_news_preprocessed, 'w') as out_news_embedded:\n",
    "    for _id, con, label, missing_words in news_preprocessed_generator():\n",
    "        out_news_embedded.write(ujson.dumps({\n",
    "            'id': _id, 'content': con, 'label': int(label)\n",
    "        }) + '\\n')\n",
    "        all_missing_words.update(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf data/fake_news_corpus/news_cleaned_2018_02_13.preprocessed.jsonl > \\\n",
    "      data/fake_news_corpus/news_cleaned_2018_02_13.embedded.preprocsessed.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lines = 0\n",
    "with open(path_news_shuffled, 'r') as in_news:\n",
    "    for line in in_news:\n",
    "        count_lines += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lines, int(count_lines * .8), int(count_lines * .1), count_lines - (int(count_lines * 0.8) + int(count_lines * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(count_lines * .8)\n",
    "test_size = int(count_lines * .8)\n",
    "val_size = count_lines - (int(count_lines * 0.8) + int(count_lines * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_news_shuffled, 'r') as in_news:\n",
    "    with open(path_news_train, 'w') as out_train:\n",
    "        with open(path_news_test, 'w') as out_test:\n",
    "            with open(path_news_val, 'w') as out_val:\n",
    "                for i, line in tqdm(enumerate(in_news)):\n",
    "                    if i < count_lines * .8:\n",
    "                        out_train.write(line)\n",
    "                    elif i < count_lines * .9:\n",
    "                        out_test.write(line)\n",
    "                    else:\n",
    "                        out_val.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _news_generator_process_line(line):\n",
    "\n",
    "    article = ujson.loads(line)\n",
    "\n",
    "    embedding = np.zeros((max_words, 100))\n",
    "    for i, word in enumerate(article['content'][:300]):\n",
    "        if word in fasttext:\n",
    "            embedding[i] = fasttext[word]\n",
    "            \n",
    "    return embedding, article['label']\n",
    "\n",
    "def news_generator(path, batch):\n",
    "    while True:\n",
    "        with open(path_news_shuffled, 'r') as in_news:\n",
    "            with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "                batch_i = 0\n",
    "                batch_embedding = np.zeros((batch, max_words, 100))\n",
    "                batch_label = np.zeros((batch, 1))\n",
    "                for embedding, label in pool.imap(_news_generator_process_line, in_news, chunksize=250):\n",
    "                    if (batch_i + 1) == batch:\n",
    "                        yield batch_embedding, batch_label\n",
    "                        batch_embedding = np.zeros((batch, max_words, 100))\n",
    "                        batch_label = np.zeros((batch, 1))\n",
    "                        batch_i = 0\n",
    "                    else:\n",
    "                        batch_embedding[batch_i] = embedding\n",
    "                        batch_label[batch_i, 0] = label\n",
    "                        batch_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN model for classyfing fake vs reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = max_words, 100\n",
    "\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_simple_model(input_shape, filters=250, kernel_size=3, hidden_dims=250):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, kernel_size, input_shape=(input_shape[0], input_shape[1]), padding='valid', \n",
    "               activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "#     model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    cnn_model = cnn_simple_model(input_shape, filters, kernel_size, hidden_dims)\n",
    "    cnn_model.fit_generator(news_generator(path_news_train, batch_size), steps_per_epoch=train_size // batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=news_generator(path_news_val, batch_size), \n",
    "                            validation_steps=val_size // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
