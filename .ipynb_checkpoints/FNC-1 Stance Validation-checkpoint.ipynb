{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import ujson\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from cleaner import Cleaner\n",
    "from tagger import Tagger\n",
    "\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/8_fnc-1/'\n",
    "train_bodies = pd.read_csv(data_path + 'train_bodies.csv')\n",
    "train_stances = pd.read_csv(data_path + 'train_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached(path, fn):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w') as _out:\n",
    "            result = fn()\n",
    "            ujson.dump(result, path)\n",
    "            return result\n",
    "    \n",
    "    with open(path, 'r') as _in:\n",
    "         return ujson.load(_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bodies_tagged = cached(data_path + 'train_bodies_tagged.json',\n",
    "                             lambda: Tagger.batch_perform(Cleaner.batch_perform(train_bodies['articleBody']), 2, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stances_tagged = cached(data_path + 'train_stances_tagged.json',\n",
    "                              lambda: Tagger.batch_perform(Cleaner.batch_perform(train_stances['Headline']), 2, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec\n",
    "https://drive.google.com/file/u/1/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('/Users/several27/GoogleNews-vectors-negative300.bin', \n",
    "                                            binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_to_w2v(tagged, model):\n",
    "    w2v = []\n",
    "    missed = 0\n",
    "    for words in tagged:\n",
    "        words_w2v = [model[w] for w in words if w in model]\n",
    "        w2v.append(words_w2v)\n",
    "        \n",
    "        missed += len(words) - len(words_w2v)\n",
    "\n",
    "    return w2v, missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bodies_w2v, train_bodies_missed_words = tagged_to_w2v(train_bodies_tagged, word2vec)\n",
    "print('Missed words', train_bodies_missed_words, '/', sum(len(ws) for ws in train_bodies_tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stances_w2v, train_stances_missed_words = tagged_to_w2v(train_stances_tagged, word2vec)\n",
    "print('Missed words', train_stances_missed_words, '/', sum(len(ws) for ws in train_stances_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct training vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_max_len = max(len(ws) for ws in train_bodies_w2v)\n",
    "stance_max_len = max(len(ws) for ws in train_stances_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_max_len, stance_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.zeros((len(train_stances_w2v), body_max_len + stance_max_len + 1, 300))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stance_id, headline, body_id, stance in tqdm(train_stances.itertuples()):\n",
    "    x_train_row = np.concatenate(\n",
    "        (np.array(train_stances_w2v[stance_id]), np.zeros((0, 300)), \n",
    "         train_bodies_w2v[train_bodies.loc[train_bodies['Body ID'] == body_id].index.values[0]]),\n",
    "        axis=0)\n",
    "    x_train[stance_id][:x_train_row.shape[0]] = x_train_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros((len(train_stances), len(set(train_stances['Stance']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_stances = list(set(train_stances['Stance']))\n",
    "for idx, stance in enumerate(train_stances['Stance']):\n",
    "    y_train[idx][possible_stances.index(stance)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv1D(filters, kernel_size, input_shape=(x_train.shape[1], x_train.shape[2]), padding='valid', \n",
    "               activation='relu', strides=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add a vanilla hidden layer:\n",
    "cnn.add(Dense(hidden_dims))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "cnn.add(Dense(4))\n",
    "cnn.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1) # validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
