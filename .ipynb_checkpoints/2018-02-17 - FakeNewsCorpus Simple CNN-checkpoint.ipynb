{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import multiprocessing\n",
    "\n",
    "import ujson\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Conv1D, GlobalMaxPooling1D, MaxPool1D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate, Dropout, Activation, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model, Model\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(500 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'data/fake_news_corpus/'\n",
    "path_news_csv = path_data + 'news_cleaned_2018_02_13.csv'\n",
    "path_fasttext = path_data + 'news_cleaned_2018_02_13.fasttext.bin'\n",
    "path_news_preprocessed = path_data + 'news_cleaned_2018_02_13.preprocessed.jsonl'\n",
    "path_news_shuffled = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.jsonl'\n",
    "path_news_train = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.train.jsonl'\n",
    "path_news_test = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.test.jsonl'\n",
    "path_news_val = path_data + 'news_cleaned_2018_02_13.preprocessed.shuffled.val.jsonl'\n",
    "\n",
    "path_news_embedded = path_data + 'news_cleaned_2018_02_13.embedded.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText.load_fasttext_format(path_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load news_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_news_chunk in pd.read_csv(path_news_csv, chunksize=1000):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the input data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the file is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm() as progress:\n",
    "    for df_news_chunk in pd.read_csv(path_news_csv, encoding='utf-8', engine='python', chunksize=1000):\n",
    "        progress.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_generator():\n",
    "    with tqdm() as progress:\n",
    "        for df_news_chunk in pd.read_csv(path_news_csv, encoding='utf-8', engine='python', chunksize=10 * 1000):\n",
    "            news_filter = df_news_chunk.type.isin(set(['fake', 'conspiracy', 'unreliable', 'reliable']))\n",
    "            df_news_chunk_filtered = df_news_chunk[news_filter]\n",
    "            for row in df_news_chunk_filtered.itertuples():\n",
    "                label = 1 if row.type == 'reliable' else 0\n",
    "\n",
    "                progress.update()\n",
    "                yield int(row.id), '%s %s' % (row.title, row.content), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show text length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for i, (_id, con, label) in enumerate(news_generator()):\n",
    "    if i > 10 * 1000:\n",
    "        break\n",
    "\n",
    "    lens.append(len(con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lens), sum(lens) / len(lens), max(lens), min(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_string(news):\n",
    "    _id, con, label = news\n",
    "    return _id, preprocess_string(con), label\n",
    "\n",
    "def news_preprocessed_generator():\n",
    "    missing_words = {}\n",
    "    \n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count(), maxtasksperchild=1) as pool:\n",
    "        for _id, con, label in pool.imap(_preprocess_string, news_generator(), chunksize=1000):\n",
    "            yield _id, con, label, missing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing_words = {}\n",
    "with open(path_news_preprocessed, 'w') as out_news_embedded:\n",
    "    for _id, con, label, missing_words in news_preprocessed_generator():\n",
    "        out_news_embedded.write(ujson.dumps({\n",
    "            'id': _id, 'content': con, 'label': int(label)\n",
    "        }) + '\\n')\n",
    "        all_missing_words.update(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf data/fake_news_corpus/news_cleaned_2018_02_13.preprocessed.jsonl > \\\n",
    "      data/fake_news_corpus/news_cleaned_2018_02_13.preprocessed.shuffled.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3937987it [01:24, 46661.80it/s]\n"
     ]
    }
   ],
   "source": [
    "count_lines = 0\n",
    "with open(path_news_shuffled, 'r') as in_news:\n",
    "    for line in tqdm(in_news):\n",
    "        count_lines += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3937987, 3150389, 393798, 393800)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_lines, int(count_lines * .8), int(count_lines * .1), \\\n",
    "    count_lines - (int(count_lines * 0.8) + int(count_lines * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(count_lines * .8)\n",
    "test_size = int(count_lines * .8)\n",
    "val_size = count_lines - (int(count_lines * 0.8) + int(count_lines * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_news_shuffled, 'r') as in_news:\n",
    "    with open(path_news_train, 'w') as out_train:\n",
    "        with open(path_news_test, 'w') as out_test:\n",
    "            with open(path_news_val, 'w') as out_val:\n",
    "                for i, line in tqdm(enumerate(in_news)):\n",
    "                    if i < count_lines * .8:\n",
    "                        out_train.write(line)\n",
    "                    elif i < count_lines * .9:\n",
    "                        out_test.write(line)\n",
    "                    else:\n",
    "                        out_val.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _news_generator_process_line(line):\n",
    "    article = ujson.loads(line)\n",
    "\n",
    "    embedding = np.zeros((max_words, 100))\n",
    "    for i, word in enumerate(article['content'][:max_words]):\n",
    "        if word in fasttext:\n",
    "            embedding[i] = fasttext[word]\n",
    "            \n",
    "    return embedding, article['label']\n",
    "\n",
    "def news_generator(path, batch):\n",
    "    while True:\n",
    "        with open(path_news_shuffled, 'r') as in_news:\n",
    "            # with multiprocessing.Pool(2) as pool:\n",
    "            batch_i = 0\n",
    "            batch_embedding = np.zeros((batch, max_words, 100))\n",
    "            batch_label = np.zeros((batch, 1))\n",
    "            for line in in_news:\n",
    "                embedding, label = _news_generator_process_line(line)\n",
    "                \n",
    "                if (batch_i + 1) == batch:\n",
    "                    yield batch_embedding, batch_label\n",
    "                    batch_embedding = np.zeros((batch, max_words, 100))\n",
    "                    batch_label = np.zeros((batch, 1))\n",
    "                    batch_i = 0\n",
    "                else:\n",
    "                    batch_embedding[batch_i] = embedding\n",
    "                    batch_label[batch_i, 0] = label\n",
    "                    batch_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN model for classyfing fake vs reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = max_words, 100\n",
    "\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_simple_model(input_shape, filters=250, kernel_size=3, hidden_dims=250):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, kernel_size, input_shape=(input_shape[0], input_shape[1]), padding='valid', \n",
    "               activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "#     model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    cnn_model = cnn_simple_model(input_shape, filters, kernel_size, hidden_dims)\n",
    "    cnn_model.fit_generator(news_generator(path_news_train, batch_size), steps_per_epoch=train_size // batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=news_generator(path_news_val, batch_size), \n",
    "                            validation_steps=val_size // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_deep_model(input_shape, filters=512, kernel_size=3, drop = 0.5, filter_sizes=(3,4,5)):\n",
    "    # https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    maxpools = []\n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Conv1D(filters, kernel_size=filter_size, padding='valid', \n",
    "                      kernel_initializer='normal', activation='relu')(inputs)\n",
    "        maxpool = MaxPool1D(pool_size=max_words - filter_size + 1, strides=1, \n",
    "                            padding='valid')(conv)\n",
    "        maxpools.append(maxpool)\n",
    "        \n",
    "    concatenated_tensor = Concatenate(axis=1)(maxpools)\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    activation = Activation('sigmoid')(dropout)\n",
    "    output = Dense(1)(activation)\n",
    "\n",
    "    # this creates a model that includes\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 9367/49224 [====>.........................] - ETA: 4059s - loss: 2.0416 - acc: 0.5684"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    cnn_model = cnn_deep_model(input_shape)\n",
    "    checkpoint = ModelCheckpoint(path_data + '1weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, \n",
    "                                 save_best_only=True, mode='auto')\n",
    "    cnn_model.fit_generator(news_generator(path_news_train, batch_size), steps_per_epoch=train_size // batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=news_generator(path_news_val, batch_size), \n",
    "                            validation_steps=val_size // batch_size, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
